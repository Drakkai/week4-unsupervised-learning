{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ML-Challenge/week4-unsupervised-learning/blob/master/L1.Clustering%20for%20dataset%20exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn how to discover the underlying groups (or \"clusters\") in a dataset. By the end of this lesson, we'll be clustering companies using their stock market prices, and distinguishing different species by clustering their measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:33.163118Z",
     "start_time": "2020-02-15T08:26:33.150125Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Download lesson datasets\n",
    "# Required if you're using Google Colab\n",
    "!wget \"https://github.com/ML-Challenge/week4-unsupervised-learning/raw/master/datasets.zip\"\n",
    "!unzip -o datasets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:33.676994Z",
     "start_time": "2020-02-15T08:26:33.166083Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import utils\n",
    "# We'll be using this module throughout the lesson\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:33.896219Z",
     "start_time": "2020-02-15T08:26:33.678965Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# and setting the size of all plots.\n",
    "plt.rcParams['figure.figsize'] = [11, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning is a class of machine learning techniques for discovering patterns in data. For instance, finding the natural \"clusters\" of customers based on their purchase histories, or searching for patterns and correlations among these purchases, and using these patterns to express the data in compressed form.\n",
    "\n",
    "These are examples of unsupervised learning techniques called \"clustering\" and \"dimension reduction\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning is defined in opposition to supervised learning. An example of supervised learning is using the measurements of tumors to classify them as benign or cancerous. In this case, the pattern discovery is guided, or \"supervised\", so that the patterns are as useful as possible for predicting the label: benign or cancerous. Unsupervised learning, in contrast, is learning without labels. It is pure pattern discovery, unguided by a prediction task.\n",
    "\n",
    "We'll start by learning about clustering but before we begin, let's introduce a dataset and fix some terminology. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris dataset consists of the measurements of many iris plants of three different species. There are four measurements: petal length, petal width, sepal length and sepal width. These are the features of the dataset.\n",
    "\n",
    "![Iris](assets/1-1.png)\n",
    "\n",
    "Throughout this lesson, datasets like this will be written as two-dimensional `numpy` arrays. The columns of the array will correspond to the features. The measurements for individual plants are the samples of the dataset. These correspond to rows of the array. \n",
    "\n",
    "The samples of the iris dataset have four measurements, and so correspond to points in a four-dimensional space. This is the dimension of the dataset.\n",
    "\n",
    "We can't visualize four dimensions directly, but using unsupervised learning techniques we can still gain insight. In this lesson, we'll cluster these samples using k-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means finds a specified number of clusters in the samples. It's implemented in the scikit-learn or \"sklearn\" library.\n",
    "\n",
    "Let's see kmeans in action on some samples from the iris dataset. To start, we import kmeans from scikit-learn.\n",
    "\n",
    "```\n",
    "from sklearn.cluster import KMeans\n",
    "```\n",
    "\n",
    "Then create a kmeans model, specifying the number of clusters we want to find. Let's specify 3 clusters, since there are three species of iris.\n",
    "\n",
    "```\n",
    "model = KMeans(n_clusters=3)\n",
    "```\n",
    "\n",
    "Now call the fit method of the model, passing the array of samples. This fits the model to the data, by locating and remembering the regions where the different clusters occur.\n",
    "\n",
    "```\n",
    "model.fit(samples)\n",
    "```\n",
    "\n",
    "Then we can use the predict method of the model on these same samples. This returns a cluster label for each sample, indicating to which cluster a sample belongs.\n",
    "\n",
    "```\n",
    "labels = model.predict(samples)\n",
    "```\n",
    "\n",
    "If someone comes along with some new iris samples, k-means can determine to which clusters they belong without starting over. k-means does this by remembering the mean (or average) of the samples in each cluster. These are called the \"centroids\". New samples are assigned to the cluster whose centroid is closest.\n",
    "\n",
    "Suppose we've got an array of new samples. To assign the new samples to the existing clusters, we pass the array of new samples to the predict method of the kmeans model. This returns the cluster labels of the new samples.\n",
    "\n",
    "```\n",
    "new_labels = model.predict(new_samples)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further on, we'll learn how to evaluate the quality of our clustering. But for now, let's visualize our clustering of the iris samples using scatter plots. \n",
    "\n",
    "Here is a scatter plot of the sepal length vs petal length of the iris samples. Each point represents an iris sample, and is colored according to the cluster of the sample. \n",
    "\n",
    "![Scatter plot](assets/1-2.png)\n",
    "\n",
    "To create a scatter plot like this, use PyPlot. Firstly, import PyPlot. It is conventionally imported as plt.\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "Now get the x- and y- co-ordinates of each sample. Sepal length is in the 0th column of the array, while petal length is in the 2nd column.\n",
    "\n",
    "```\n",
    "xs = samples[:,0]\n",
    "ys = samples[:,2]\n",
    "```\n",
    "\n",
    "Now call the plt dot scatter function, passing the x- and y-co-ordinates and specifying c=labels to colour by cluster label.\n",
    "\n",
    "```\n",
    "plt.scatter(xs, ys, c=labels)\n",
    "```\n",
    "\n",
    "When we are ready to show the plot, we call `plt.show`.\n",
    "\n",
    "```\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering 2D points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given an array `points` of size 300x2, where each row gives the (x, y) co-ordinates of a point on a map. Let's make a scatter plot of these points, and use the scatter plot to guess how many clusters there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:33.912177Z",
     "start_time": "2020-02-15T08:26:33.897218Z"
    }
   },
   "outputs": [],
   "source": [
    "xs = utils.points[:, 0]\n",
    "ys = utils.points[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:34.051837Z",
     "start_time": "2020-02-15T08:26:33.914171Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plot above, we can see that the points seem to separate into 3 clusters. We'll now create a KMeans model to find 3 clusters, and fit it to our data points. After the model has been fit, we'll obtain the cluster labels for some new points using the `.predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:34.535857Z",
     "start_time": "2020-02-15T08:26:34.053850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import KMeans\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:34.550744Z",
     "start_time": "2020-02-15T08:26:34.538777Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a KMeans instance with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:34.581662Z",
     "start_time": "2020-02-15T08:26:34.552738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit model to points\n",
    "model.fit(utils.points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:34.596621Z",
     "start_time": "2020-02-15T08:26:34.583655Z"
    }
   },
   "outputs": [],
   "source": [
    "# Determine the cluster labels of new_points: labels\n",
    "labels = model.predict(utils.new_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:34.612578Z",
     "start_time": "2020-02-15T08:26:34.598616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print cluster labels of new_points\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've successfully performed k-Means clustering and predicted the labels of new points. But it is not easy to inspect the clustering by just looking at the printed labels. A visualization would be far more useful. In the next example, we'll inspect our clustering with a scatter plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect the clustering we performed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:34.627541Z",
     "start_time": "2020-02-15T08:26:34.614575Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assign the columns of new_points: xs and ys\n",
    "xs = utils.new_points[:,0]\n",
    "ys = utils.new_points[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:34.642498Z",
     "start_time": "2020-02-15T08:26:34.628568Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assign the cluster centers: centroids\n",
    "centroids = model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:34.657459Z",
     "start_time": "2020-02-15T08:26:34.643496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assign the columns of centroids: centroids_x, centroids_y\n",
    "centroids_x = centroids[:,0]\n",
    "centroids_y = centroids[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:34.844956Z",
     "start_time": "2020-02-15T08:26:34.661448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a scatter plot of xs and ys, using labels to define the colors\n",
    "plt.scatter(xs, ys, alpha=0.5, c=labels)\n",
    "\n",
    "# Make a scatter plot of centroids_x and centroids_y\n",
    "plt.scatter(centroids_x, centroids_y, marker='D', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering looks great! But how can we be sure that 3 clusters is the correct choice? In other words, how can we evaluate the quality of a clustering? Next, we will explain how to evaluate a clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used k-means to cluster the iris samples into three clusters. But how can we evaluate the quality of this clustering?\n",
    "\n",
    "A direct approach is to compare the clusters with the iris species. We'll learn about this first, before considering the problem of how to measure the quality of a clustering in a way that doesn't require our samples to come pre-grouped into species.\n",
    "\n",
    "This measure of quality can then be used to make an informed choice about the number of clusters to look for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris: clusters vs species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's check whether the 3 clusters of iris samples have any correspondence to the iris species. The correspondence is described by this table.\n",
    "\n",
    "\n",
    "| species labels | setosa | versicolor | virginica |\n",
    "| :------------- | -----: | ---------: | --------: |\n",
    "| 0              | 0      | 2          | 36        |\n",
    "| 1              | 50     | 0          | 0         |\n",
    "| 2              | 0      | 48         | 14        |\n",
    "\n",
    "There is one column for each of the three species of iris: setosa,versicolor and virginica, and one row for each of the three cluster labels: 0, 1 and 2. \n",
    "\n",
    "The table shows the number of samples that have each possible cluster label/species combination. For example, we see that cluster 1 corresponds perfectly with the species setosa. On the other hand, while cluster 0 contains mainly virginica samples, there are also some virginica samples in cluster 2. Tables like these are called \"cross-tabulations\". To construct one, we are going to use the pandas library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross tabulation with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume the species of each sample is given as a list of strings. \n",
    " \n",
    "Import pandas, and then create a two-column dataframe, where the first column is cluster labels and the second column is the iris species, so that each row gives the cluster label and species of a single sample.\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "df = pandas.DataFrame({'labels': labels, 'species': species})\n",
    "```\n",
    "\n",
    "Now use the pandas `crosstab` function to build the cross tabulation, passing the two columns of the dataframe.\n",
    "\n",
    "```\n",
    "ct = pd.crosstab(df['labels'], df['species'])\n",
    "\n",
    "print(ct)\n",
    "```\n",
    "\n",
    "Cross tabulations like these provide great insights into which sort of samples are in which cluster. \n",
    "\n",
    "But in most datasets, the samples are not labeled by species. How can the quality of a clustering be evaluated in these cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring clustering quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to measure the quality of a clustering that uses only the clusters and the samples themselves. A good clustering has tight clusters, meaning that the samples in each cluster are bunched together, not spread out.\n",
    "\n",
    "How spread out the samples within each cluster are can be measured by the \"inertia\". Intuitively, inertia measures how far samples are from their centroids. We can find the precise definition in the scikit-learn documentation. We want clusters that are not spread out, so lower values of the inertia are better.\n",
    "\n",
    "The inertia of a kmeans model is measured automatically when any of the fit methods are called, and is available afterwards as the `inertia_` attribute.\n",
    "\n",
    "In fact, kmeans aims to place the clusters in a way that minimizes the inertia. Here is a plot of the inertia values of clusterings of the iris dataset with different numbers of clusters.\n",
    "\n",
    "![Inertia](assets/1-3.png)\n",
    "\n",
    "Our kmeans model with 3 clusters has relatively low inertia, which is great. But notice that the inertia continues to decrease slowly. So what's the best number of clusters to choose? Ultimately, this is a trade-off.\n",
    "\n",
    "A good clustering has tight clusters (meaning low inertia). But it also doesn't have too many clusters. A good rule of thumb is to choose an elbow in the inertia plot, that is, a point where the inertia begins to decrease more slowly. For example, by this criterion, 3 is a good number of clusters for the iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many clusters of grain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we learned how to choose a good number of clusters for a dataset using the k-means inertia graph. We are given an array `grains` containing the measurements of samples of grain, such as area, perimeter, length, and several others. What's a good number of clusters in this case?\n",
    "\n",
    "This dataset was sourced from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/seeds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:34.969930Z",
     "start_time": "2020-02-15T08:26:34.846951Z"
    }
   },
   "outputs": [],
   "source": [
    "ks = range(1, 6)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(utils.grain)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.080503Z",
     "start_time": "2020-02-15T08:26:34.972923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot ks vs inertias\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inertia decreases very slowly from 3 clusters to 4, so it looks like 3 clusters would be a good choice for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the grain clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we observed from the inertia plot that 3 is a good number of clusters for the grain data. In fact, the grain samples come from a mix of 3 different grain varieties: \"Kama\", \"Rosa\" and \"Canadian\". In this example, we cluster the grain samples into three clusters, and compare the clusters to the grain varieties using a cross-tabulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.096457Z",
     "start_time": "2020-02-15T08:26:35.082495Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a KMeans model with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.128425Z",
     "start_time": "2020-02-15T08:26:35.097455Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use fit_predict to fit model and obtain cluster labels: labels\n",
    "labels = model.fit_predict(utils.grain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.144328Z",
     "start_time": "2020-02-15T08:26:35.129368Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with labels and varieties as columns: df\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': utils.varieties})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.175310Z",
     "start_time": "2020-02-15T08:26:35.146354Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-tabulation shows that the 3 varieties of grain separate really well into 3 clusters. But depending on the type of data we are working with, the clustering may not always be this good. Is there anything we can do in such situations to improve the clustering? We'll find out in the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming features for better clusterings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look now at another dataset: the Piedmont wines dataset. We have 178 samples of red wine from the Piedmont region of Italy. The features measure chemical composition, like alcohol content, and visual properties like color intensity. The samples come from 3 distinct varieties of wine.\n",
    "\n",
    "Let's take the array of samples and use KMeans to find 3 clusters.\n",
    "\n",
    "```\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "labels.model.fit_predict(samples)\n",
    "```\n",
    "\n",
    "There are three varieties of wine, so let's use pandas `crosstab` to check the cluster label - wine variety correspondence.\n",
    "\n",
    "```\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "ct = pd.crosstab(df['labels']. df['varieties'])\n",
    "\n",
    "print(ct)\n",
    "```\n",
    "\n",
    "| varieties labels | Barbera | Barolo | Grignolino |\n",
    "| :--------------- | -----: | ------: | ---------: |\n",
    "| 0                | 0      | 2       | 36         |\n",
    "| 1                | 50     | 0       | 0          |\n",
    "| 2                | 0      | 48      | 14         |\n",
    "\n",
    "As we can see, this time things haven't worked out so well. The KMeans clusters don't correspond well with the wine varieties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that the features of the wine dataset have very different variances. The variance of a feature measures the spread of its values. For example, the `malic` acid feature has a higher variance than the `od280` feature, and this can also be seen in their scatter plot. \n",
    "\n",
    "![Feature variance](assets/1-4.png)\n",
    "\n",
    "The differences in some of the feature variances is enormous, as seen here, for example, in the scatter plot of the od280 and proline features.\n",
    "\n",
    "![Feature variance](assets/1-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In KMeans clustering, the variance of a feature corresponds to its influence on the clustering algorithm. \n",
    "\n",
    "To give every feature a chance, the data needs to be transformed so that features have equal variance. This can be achieved with the `StandardScaler` from scikit-learn. It transforms every feature to have mean 0 and variance 1. The resulting \"standardized\" features can be very informative.\n",
    "\n",
    "Using standardized `od280` and `proline`, for example, the three wine varieties are much more distinct.\n",
    "\n",
    "![Standardized](assets/1-6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the StandardScaler in action. First, import StandardScaler from sklearn.preprocessing.\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "\n",
    "Then create a StandardScaler object, and fit it to the samples.\n",
    "\n",
    "```\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(samples)\n",
    "```\n",
    "\n",
    "The transform method can now be used to standardize any samples, either the same ones, or completely new ones.\n",
    "\n",
    "```\n",
    "samples_scaled = scaler.transform(samples)\n",
    "```\n",
    "\n",
    "The APIs of StandardScaler and KMeans are similar, but there is an important difference. StandardScaler transforms data, and so has a transform method. KMeans, in contrast, assigns cluster labels to samples, and this is done using the predict method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the problem of clustering the wines. We need to perform two steps. Firstly, to standardize the data using StandardScaler, and secondly to take the standardized data and cluster it using KMeans. This can be conveniently achieved by combining the two steps using a scikit-learn pipeline. Data then flows from one step into the next, automatically.\n",
    "\n",
    "The first steps are the same: creating a StandardScaler and a KMeans object.\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "scaler = StandardScaler()\n",
    "kmeans = KMeans(n_clusters = 3)\n",
    "```\n",
    "\n",
    "After that, import the make_pipeline function from sklearn.pipeline.\n",
    "\n",
    "```\n",
    "from sklearn.pipeline import make_pipeline\n",
    "```\n",
    "\n",
    "Apply the make_pipeline function to the steps that we want to compose in this case, the scaler and the kmeans objects.\n",
    "\n",
    "```\n",
    "pipeline = make_pipeline(scaler, kmeans)\n",
    "```\n",
    "\n",
    "Now use the fit method of the pipeline to fit both the scaler and kmeans, and use its predict method to obtain the cluster labels.\n",
    "\n",
    "```\n",
    "pipeline.fit(samples)\n",
    "\n",
    "labels = pipeline.predict(samples)\n",
    "```\n",
    "\n",
    "Checking the correspondence between the cluster labels and the wine varieties reveals that this new clustering, incorporating standardization, is fantastic. Its three clusters correspond almost exactly to the three wine varieties. This is a huge improvement on the clustering without standardization.\n",
    "\n",
    "```\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "```\n",
    "\n",
    "| varieties labels | Barbera | Barolo | Grignolino |\n",
    "| :--------------- | -----: | ------: | ---------: |\n",
    "| 0                | 0      | 59      | 3          |\n",
    "| 1                | 48     | 0       | 3          |\n",
    "| 2                | 0      | 0       | 65         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScaler is an example of a \"preprocessing\" step. There are several of these available in scikit-learn, for example `MaxAbsScaler` and `Normalizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling fish data for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given an array `fish_samples` giving measurements of fish. Each row represents an individual fish. The measurements, such as weight in grams, length in centimeters, and the percentage ratio of height to length, have very different scales. In order to cluster this data effectively, we'll need to standardize these features first. In this example, we'll build a pipeline to standardize and cluster the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These fish measurement data were sourced from the [Journal of Statistics Education](http://jse.amstat.org/jse_data_archive.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.191236Z",
     "start_time": "2020-02-15T08:26:35.177271Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.207217Z",
     "start_time": "2020-02-15T08:26:35.195254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.223149Z",
     "start_time": "2020-02-15T08:26:35.210184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.239127Z",
     "start_time": "2020-02-15T08:26:35.225142Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Clustering the fish data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll now use the standardization and clustering pipeline from the previous example to cluster the fish by their measurements, and then create a cross-tabulation to compare the cluster labels with the fish species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.286003Z",
     "start_time": "2020-02-15T08:26:35.241103Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline to samples\n",
    "pipeline.fit(utils.fish_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.301970Z",
     "start_time": "2020-02-15T08:26:35.287975Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Calculate the cluster labels: labels\n",
    "labels = pipeline.predict(utils.fish_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.316899Z",
     "start_time": "2020-02-15T08:26:35.303963Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with labels and species as columns: df\n",
    "df = pd.DataFrame({'labels':labels, 'species':utils.species})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.347849Z",
     "start_time": "2020-02-15T08:26:35.318893Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'], df['species'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It looks like the fish data separates really well into 4 clusters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Clustering stocks using KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example, we'll cluster companies using their daily stock price movements (i.e. the dollar difference between the closing and opening prices for each trading day). We have a NumPy array `movements` of daily price movements from 2010 to 2015 (obtained from Yahoo! Finance), where each row corresponds to a company, and each column corresponds to a trading day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some stocks are more expensive than others. To account for this, we include a `Normalizer` at the beginning of the pipeline. The Normalizer will separately transform each company's stock price to a relative scale before the clustering begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that `Normalizer()` is different to `StandardScaler()`, which we used in the previous example. While `StandardScaler()` standardizes **features** (such as the features of the fish data from the previous example) by removing the mean and scaling to unit variance, `Normalizer()` rescales **each sample** - here, each company's stock price - independently of the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.362800Z",
     "start_time": "2020-02-15T08:26:35.349811Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import Normalizer\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.378757Z",
     "start_time": "2020-02-15T08:26:35.364769Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a normalizer: normalizer\n",
    "normalizer = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.394714Z",
     "start_time": "2020-02-15T08:26:35.380745Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a KMeans model with 10 clusters: kmeans\n",
    "kmeans = KMeans(n_clusters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.410648Z",
     "start_time": "2020-02-15T08:26:35.398678Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make a pipeline chaining normalizer and kmeans: pipeline\n",
    "pipeline = make_pipeline(normalizer, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.519357Z",
     "start_time": "2020-02-15T08:26:35.412673Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fit pipeline to the daily price movements\n",
    "pipeline.fit(utils.movements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Which stocks move together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the previous example, we clustered companies by their daily stock price movements. So which company have stock prices that tend to change in the same way? We'll now inspect the cluster labels from the clustering to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.535344Z",
     "start_time": "2020-02-15T08:26:35.522346Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Predict the cluster labels: labels\n",
    "labels = pipeline.predict(utils.movements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:35.551274Z",
     "start_time": "2020-02-15T08:26:35.537308Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame aligning labels and companies: df\n",
    "df = pd.DataFrame({'labels': labels, 'companies': utils.companies})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T08:26:42.361862Z",
     "start_time": "2020-02-15T08:26:42.338936Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Display df sorted by cluster label\n",
    "print(df.sort_values('labels'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Take a look at the clusters. Are any of the results surprising? In the next lesson, we'll learn about how to communicate results such as this through visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "**[Week 4 - Unsupervised Learning](https://radu-enuca.gitbook.io/ml-challenge/unsupervised-learning)**\n",
    "\n",
    "*Have questions or comments? Visit the ML Challenge Mattermost Channel.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
